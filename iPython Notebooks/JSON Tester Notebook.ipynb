{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This noteboook is designed as a prototype script for grabbing articles from the JSON file.\n",
    "\n",
    "First let's create a small batch of the JSON objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pathway = \"/Volumes/Seagate Backup Plus Drive 1/RC_2015-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import shelve\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords.append('[deleted]')\n",
    "stopwords.append('deleted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now', '[deleted]', 'deleted']\n"
     ]
    }
   ],
   "source": [
    "print stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to quickly make a python shelve database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def json_to_db(db_name,num_of_comments):\n",
    "    \n",
    "\n",
    "    '''\n",
    "    Goes through the txt file, grabs the JSON objects in it.\n",
    "    Uses shelve to create a database, with key= Subreddit and continually appending text to value.\n",
    "    '''\n",
    "    # open database\n",
    "    d = shelve.open(db_name)\n",
    "    \n",
    "    # Set counter\n",
    "    count = 0\n",
    "    \n",
    "    # Iterate through N JSON objects in file\n",
    "    with open(pathway) as myfile:\n",
    "        for item in myfile:\n",
    "            if count < num_of_comments:\n",
    "                \n",
    "                # Load the JSON object\n",
    "                json_object = json.loads(item)\n",
    "                \n",
    "                # Now append additional text to database\n",
    "                if d.has_key(json_object['subreddit']):\n",
    "                    \n",
    "                    temp = d[json_object['subreddit']]      # extracts the copy\n",
    "                    temp = temp+' '+json_object['body']     # mutates the copy\n",
    "                    d[json_object['subreddit']] = temp      # stores the copy right back, to persist it\n",
    "\n",
    "                # Or set the first entry for that subreddit\n",
    "                else:\n",
    "                    d[json_object['subreddit']] = json_object['body']\n",
    "                \n",
    "    \n",
    "                count += 1\n",
    "            else:\n",
    "                d.close()\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_to_db('10000_comments',10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = shelve.open('10000_comments.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"Thanks! Now on to NYE makeup! [deleted] Right!? My face loves me, my wallet hates me.  Simple and Tasteful. :) Dipbrow is definitely something that has to be learned haha. I just barely stick the end of my brush into the pot and thats usually enough to cover my whole brow. But I also use the brow wiz first to outline my brows.  They look great on you, especially the red. How much do these cost if you don't mind me asking? Yeah she did that in high school...I've also recently read that she lied about being molested. Not sure how I feel about her now especially since she hardly does beauty vids anymore. She def is lacking when it comes to effort in her vids. I mixed a little of a Tarte stain (berry colored) with the baby lips clear chapstick, I've had the Tarte stain forever and the label is worn off so i dont know the exact color but i'll try and find out! and thank you!! Seriously. The recommendations for Gerard cosmetics need to chill the fuck out. Sona Gasparian's no makeup-makeup tutorial is the best one I've ever seen. Her YouTube username is makeupbysona.  You look awesome! I wish my eyebrows were that nice \\U0001f61e OH! well they're great and have a ton of eyelashes for super cheap I believe these ones are [jo jo](http://ohmy-lash.com/collections/new-sweetheart-collectiion/products/jo-jo) from the sweetheart collection, and it looks like they took down the subscription only option so you can \\nbuy them individually again! It doesn't take much water, I get my fingers wet with hot water and gently press my fingers together over my lashes.  No pulling, just touching them together around the lashes.  Agreed. Beauty gurus are lacking beauty vids and make up for it with hauls, q&amp;a's, sponsored vids, and vlogs. Those are cool every once in awhile but I watch beauty vids for help with beauty techniques, tutorials, and reviews on products I'm interested in. oh no! I love her, I hope it's just rumors :( Thanks! Now on to NYE makeup! [deleted] Right!? My face loves me, my wallet hates me.  Simple and Tasteful. :) Dipbrow is definitely something that has to be learned haha. I just barely stick the end of my brush into the pot and thats usually enough to cover my whole brow. But I also use the brow wiz first to outline my brows.  They look great on you, especially the red. How much do these cost if you don't mind me asking? Yeah she did that in high school...I've also recently read that she lied about being molested. Not sure how I feel about her now especially since she hardly does beauty vids anymore. She def is lacking when it comes to effort in her vids. I mixed a little of a Tarte stain (berry colored) with the baby lips clear chapstick, I've had the Tarte stain forever and the label is worn off so i dont know the exact color but i'll try and find out! and thank you!! Seriously. The recommendations for Gerard cosmetics need to chill the fuck out. Sona Gasparian's no makeup-makeup tutorial is the best one I've ever seen. Her YouTube username is makeupbysona.  You look awesome! I wish my eyebrows were that nice \\U0001f61e OH! well they're great and have a ton of eyelashes for super cheap I believe these ones are [jo jo](http://ohmy-lash.com/collections/new-sweetheart-collectiion/products/jo-jo) from the sweetheart collection, and it looks like they took down the subscription only option so you can \\nbuy them individually again! It doesn't take much water, I get my fingers wet with hot water and gently press my fingers together over my lashes.  No pulling, just touching them together around the lashes.  Agreed. Beauty gurus are lacking beauty vids and make up for it with hauls, q&amp;a's, sponsored vids, and vlogs. Those are cool every once in awhile but I watch beauty vids for help with beauty techniques, tutorials, and reviews on products I'm interested in. oh no! I love her, I hope it's just rumors :(\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['MakeupAddiction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Cleaning Text\n",
    "\n",
    "Here we focus on converting the large string into a list of words (tokenized and stopwords removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Output tokenizes text and removes any stopwords and then outptus lowercased words\n",
    "output = [word.lower() for word in tokenizer.tokenize(d['MakeupAddiction']) if not word.lower() in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "722"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.tokenize(d['MakeupAddiction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final word cleaner code is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def text_cleaner(text):\n",
    "    '''\n",
    "    INPUT: string of body text\n",
    "    OUTPUT: List of tokenized lower case words with stopwords removed\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # Output tokenizes text and removes any stopwords and then outptus lowercased words\n",
    "    return [word.lower() for word in tokenizer.tokenize(text) if not word.lower() in stopwords]\n",
    "    \n",
    "#text_cleaner(d['MakeupAddiction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MONGO DB Setup\n",
    "\n",
    "This focuses on setting up a MongoDB and Push the new body of comments to the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "# Connect with db\n",
    "#reddit_data = pymongo.Connection()['reddit_database']['comments']\n",
    "\n",
    "def update_mongo(collection,subreddit,comment):\n",
    "    '''\n",
    "    INPUT: subreddit name and comment as a cleaned list of words\n",
    "    RESULT: Updates MongoDB reddit_data database with $push\n",
    "    '''\n",
    "    \n",
    "    # NOTE: This pushes the entire list, so final result is a list of comment lists\n",
    "    # Use itertools.chain.from_iterable(reddit_data[subreddit]) to combine this for doc2vec\n",
    "    collection.update({'subreddit': subreddit},\n",
    "                      {'$push':{'body':comment}},upsert = True,\n",
    "                      safe=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comment_test = text_cleaner(d['MakeupAddiction'])\n",
    "update_mongo(reddit_data,'MakeupAddiction',comment_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clears database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clear_mongo(mongodb):\n",
    "    '''\n",
    "    INPUT: A mongodb database\n",
    "    RESULT: Based on user input, clears database.\n",
    "    '''\n",
    "    ans = raw_input('WARNING: THIS WILL CLEAR THE DATABASE. ARE YOU SURE? y/n')\n",
    "    if ans == 'y':\n",
    "        mongodb.remove()\n",
    "        print 'Database has been cleared.'\n",
    "    else:\n",
    "        print 'Database not cleared.'\n",
    "        \n",
    "        \n",
    "#clear_mongo(reddit_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take in SubReddit, return document list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools \n",
    "\n",
    "def get_doc_list(db_collection,subreddit):\n",
    "    '''\n",
    "    INPUT: Subreddit name\n",
    "    OUTPUT: A document = list of words for that subreddit\n",
    "    '''\n",
    "    \n",
    "    for obj in db_collection.find({'subreddit':subreddit}):\n",
    "        return list(itertools.chain.from_iterable(obj['body']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'roofers',\n",
       " u'people',\n",
       " u'job',\n",
       " u'site',\n",
       " u'savage',\n",
       " u'rock',\n",
       " u'people',\n",
       " u'know',\n",
       " u'100',\n",
       " u'delete',\n",
       " u'comments',\n",
       " u'wrong',\n",
       " u'something',\n",
       " u'like',\n",
       " u'misread',\n",
       " u'question',\n",
       " u'answered',\n",
       " u'misread',\n",
       " u'misinterpreted',\n",
       " u'something',\n",
       " u'm',\n",
       " u'going',\n",
       " u'delete',\n",
       " u'got',\n",
       " u'299',\n",
       " u'489242049t4bit43',\n",
       " u'downvotes',\n",
       " u'tried',\n",
       " u'turning',\n",
       " u'great',\n",
       " u'power',\n",
       " u'6',\n",
       " u'7',\n",
       " u'grandfather',\n",
       " u'came',\n",
       " u'germany',\n",
       " u'around',\n",
       " u'1776',\n",
       " u'ship',\n",
       " u'ride',\n",
       " u'mother',\n",
       " u'dies',\n",
       " u'two',\n",
       " u'weeks',\n",
       " u'boat',\n",
       " u'father',\n",
       " u'dies',\n",
       " u'around',\n",
       " u'9',\n",
       " u'years',\n",
       " u'old',\n",
       " u'sold',\n",
       " u'indigent',\n",
       " u'servant',\n",
       " u'earned',\n",
       " u'independence',\n",
       " u'fighting',\n",
       " u'revolutionary',\n",
       " u'war',\n",
       " u'moved',\n",
       " u'ohio',\n",
       " u'near',\n",
       " u'pittsburgh',\n",
       " u'lived',\n",
       " u'101',\n",
       " u'years',\n",
       " u'old',\n",
       " u'r',\n",
       " u'firstworldavjafmasdlfja']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_doc_list(reddit_data,'AskReddit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a MongoDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "def create_or_connect(database,collection):\n",
    "    '''\n",
    "    INPUT: Database name (str) and collection (name)\n",
    "    OUTPUT: Either creates the database or connects to it if it already exists.\n",
    "            Returns the collections in the database cursor.\n",
    "    '''\n",
    "    \n",
    "    return pymongo.Connection()[database][collection]\n",
    "\n",
    "#create_or_connect('check','test_coll') works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MongoDB from my Reddit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: THIS WILL CLEAR THE DATABASE. ARE YOU SURE? y/ny\n",
      "Database has been cleared.\n"
     ]
    }
   ],
   "source": [
    "# Clear the db first\n",
    "clear_mongo(reddit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make or Connect to Database\n",
    "reddit_data = create_or_connect('reddit_database','reddit_comments')\n",
    "\n",
    "def reddit_data_pusher(db_name,num_of_comments):\n",
    "    \n",
    "    '''\n",
    "    Goes through the txt file, grabs the JSON objects in it.\n",
    "    Uses MongoDB to create a database, with key= Subreddit and continually appending clean text word list to value.\n",
    "    '''\n",
    "    \n",
    "    # Set counter\n",
    "    count = 0\n",
    "    \n",
    "    # Iterate through N JSON objects in file\n",
    "    with open(pathway) as myfile:\n",
    "        for item in myfile:\n",
    "            if count < num_of_comments:\n",
    "                \n",
    "                # Load the JSON object\n",
    "                json_object = json.loads(item)\n",
    "                \n",
    "                # Clean and tokenize text\n",
    "                body = text_cleaner(json_object['body'])\n",
    "                \n",
    "                # $push to MongoDB\n",
    "                update_mongo(reddit_data,json_object['subreddit'],body)\n",
    "                \n",
    "                # For safety puposes\n",
    "                count += 1\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a MongoDB with N comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: THIS WILL CLEAR THE DATABASE. ARE YOU SURE? y/ny\n",
      "Database has been cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_mongo(reddit_data)\n",
    "reddit_data = create_or_connect('reddit_database','reddit_comments')\n",
    "reddit_data_pusher(reddit_data,100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Doc2Vec Model Training\n",
    "\n",
    "Let's try to train a model on a very small corpus (10,000 comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1288859"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "import gensim.models.doc2vec\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1\n",
    "\n",
    "# Doc2Vec(dbow,d100,n5,mc2,t8)\n",
    "d2v_reddit_model = Doc2Vec( dm=0, size=100, negative=5, hs=0, min_count=2, workers=cores)\n",
    "d2v_reddit_model.build_vocab(sentence_gen(reddit_data))\n",
    "d2v_reddit_model.train(sentence_gen(reddit_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('IraqiSoccer', 0.554364025592804),\n",
       " ('wowaddons', 0.5370263457298279),\n",
       " ('Bikeporn', 0.5362747311592102),\n",
       " ('CemeteryPorn', 0.5331411957740784),\n",
       " ('killthosewhodisagree', 0.5311126708984375),\n",
       " ('DontPanic', 0.5282922387123108),\n",
       " ('kayakfishing', 0.5275236964225769),\n",
       " ('Warships', 0.5217361450195312),\n",
       " ('Revolvers', 0.5176466703414917),\n",
       " ('reckful', 0.5163941383361816)]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_reddit_model.most_similar('MilitaryPorn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Generator of Labeled Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentence_gen(database):\n",
    "    for item in database.find():\n",
    "        words=list(itertools.chain.from_iterable(item['body']))\n",
    "        yield LabeledSentence(words,labels=[str(item['subreddit'])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qq = LabeledSentence(words=['alpha','beta','charlie'],labels=['first'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nfor epoch in range(10):\\n    model.train(sentence_gen(reddit_data))\\n    model.alpha -= 0.002  # decrease the learning rate\\n    model.min_alpha = model.alpha  # fix the learning rate, no decay\\n'"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Doc2Vec(alpha=0.025, min_alpha=0.025)  # use fixed learning rate\n",
    "model.build_vocab(sentence_gen(reddit_data))\n",
    "model.train(sentence_gen(reddit_data))\n",
    "''' \n",
    "for epoch in range(10):\n",
    "    model.train(sentence_gen(reddit_data))\n",
    "    model.alpha -= 0.002  # decrease the learning rate\n",
    "    model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    d2v_reddit_model.train(sentence_gen(reddit_data))\n",
    "    d2v_reddit_model.alpha -= 0.002  # decrease the learning rate\n",
    "    d2v_reddit_model.min_alpha = model.alpha  # fix the learning rate, no decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
