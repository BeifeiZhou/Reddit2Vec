{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Necessary packages onto Amazon EC2 Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading/unpacking gensim\n",
      "  Downloading gensim-0.12.1.tar.gz (2.3Mb): 2.3Mb downloaded\n",
      "  Running setup.py egg_info for package gensim\n",
      "    \n",
      "    warning: no files found matching '*.sh' under directory '.'\n",
      "    no previously-included directories found matching 'docs/src*'\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy>=1.3 in /usr/local/lib/python2.7/dist-packages (from gensim)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scipy>=0.7.0 in /usr/lib/python2.7/dist-packages (from gensim)\n",
      "Downloading/unpacking six>=1.2.0 (from gensim)\n",
      "  Downloading six-1.9.0.tar.gz\n",
      "  Running setup.py egg_info for package six\n",
      "    \n",
      "    no previously-included directories found matching 'documentation/_build'\n",
      "Downloading/unpacking smart-open>=1.2.1 (from gensim)\n",
      "  Downloading smart_open-1.2.1.tar.gz\n",
      "  Running setup.py egg_info for package smart-open\n",
      "    \n",
      "Downloading/unpacking boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
      "  Downloading boto-2.38.0.tar.gz (1.4Mb): 1.4Mb downloaded\n",
      "  Running setup.py egg_info for package boto\n",
      "    \n",
      "    warning: no files found matching 'boto/mturk/test/*.doctest'\n",
      "    warning: no files found matching 'boto/mturk/test/.gitignore'\n",
      "Downloading/unpacking bz2file (from smart-open>=1.2.1->gensim)\n",
      "  Downloading bz2file-0.98.tar.gz\n",
      "  Running setup.py egg_info for package bz2file\n",
      "    \n",
      "Installing collected packages: gensim, six, smart-open, boto, bz2file\n",
      "  Running setup.py install for gensim\n",
      "    \n",
      "    warning: no files found matching '*.sh' under directory '.'\n",
      "    no previously-included directories found matching 'docs/src*'\n",
      "    building 'gensim.models.word2vec_inner' extension\n",
      "    gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -I/home/ubuntu/notebooks/build/gensim/gensim/models -I/usr/include/python2.7 -I/usr/local/lib/python2.7/dist-packages/numpy/core/include -c ./gensim/models/word2vec_inner.c -o build/temp.linux-x86_64-2.7/./gensim/models/word2vec_inner.o\n",
      "    /usr/include/python2.7/numpy/__multiarray_api.h:1532:1: warning: ���_import_array��� defined but not used [-Wunused-function]\n",
      "    /usr/include/python2.7/numpy/__ufunc_api.h:226:1: warning: ���_import_umath��� defined but not used [-Wunused-function]\n",
      "    ./gensim/models/word2vec_inner.c: In function ���__pyx_pf_6gensim_6models_14word2vec_inner_train_sentence_sg.isra.14���:\n",
      "    ./gensim/models/word2vec_inner.c:2052:65: warning: ���__pyx_v_syn1neg��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/word2vec_inner.c:3030:52: note: ���__pyx_v_syn1neg��� was declared here\n",
      "    ./gensim/models/word2vec_inner.c:1994:151: warning: ���__pyx_v_cum_table_len��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/word2vec_inner.c:3032:25: note: ���__pyx_v_cum_table_len��� was declared here\n",
      "    ./gensim/models/word2vec_inner.c:1994:151: warning: ���__pyx_v_cum_table��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/word2vec_inner.c:3031:28: note: ���__pyx_v_cum_table��� was declared here\n",
      "    ./gensim/models/word2vec_inner.c:1870:58: warning: ���__pyx_v_next_random��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/word2vec_inner.c:3033:25: note: ���__pyx_v_next_random��� was declared here\n",
      "    ./gensim/models/word2vec_inner.c:1664:65: warning: ���__pyx_v_syn1��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/word2vec_inner.c:3027:52: note: ���__pyx_v_syn1��� was declared here\n",
      "    ./gensim/models/word2vec_inner.c: In function ���__pyx_pf_6gensim_6models_14word2vec_inner_2train_sentence_cbow.isra.13���:\n",
      "    ./gensim/models/word2vec_inner.c:2764:65: warning: ���__pyx_v_syn1neg��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/word2vec_inner.c:4043:52: note: ���__pyx_v_syn1neg��� was declared here\n",
      "    ./gensim/models/word2vec_inner.c:2706:151: warning: ���__pyx_v_cum_table_len��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/word2vec_inner.c:4045:25: note: ���__pyx_v_cum_table_len��� was declared here\n",
      "    ./gensim/models/word2vec_inner.c:2706:151: warning: ���__pyx_v_cum_table��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/word2vec_inner.c:4044:28: note: ���__pyx_v_cum_table��� was declared here\n",
      "    ./gensim/models/word2vec_inner.c:1870:58: warning: ���__pyx_v_next_random��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/word2vec_inner.c:4046:25: note: ���__pyx_v_next_random��� was declared here\n",
      "    ./gensim/models/word2vec_inner.c:2334:65: warning: ���__pyx_v_syn1��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/word2vec_inner.c:4040:52: note: ���__pyx_v_syn1��� was declared here\n",
      "    gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro build/temp.linux-x86_64-2.7/./gensim/models/word2vec_inner.o -o build/lib.linux-x86_64-2.7/gensim/models/word2vec_inner.so\n",
      "    building 'gensim.models.doc2vec_inner' extension\n",
      "    gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -I/home/ubuntu/notebooks/build/gensim/gensim/models -I/usr/include/python2.7 -I/usr/local/lib/python2.7/dist-packages/numpy/core/include -c ./gensim/models/doc2vec_inner.c -o build/temp.linux-x86_64-2.7/./gensim/models/doc2vec_inner.o\n",
      "    /usr/include/python2.7/numpy/__multiarray_api.h:1532:1: warning: ���_import_array��� defined but not used [-Wunused-function]\n",
      "    /usr/include/python2.7/numpy/__ufunc_api.h:226:1: warning: ���_import_umath��� defined but not used [-Wunused-function]\n",
      "    ./gensim/models/doc2vec_inner.c: In function ���__pyx_pf_6gensim_6models_13doc2vec_inner_2train_document_dm.isra.12���:\n",
      "    ./gensim/models/doc2vec_inner.c:2166:65: warning: ���__pyx_v_syn1neg��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/doc2vec_inner.c:4363:52: note: ���__pyx_v_syn1neg��� was declared here\n",
      "    ./gensim/models/doc2vec_inner.c:2108:151: warning: ���__pyx_v_cum_table_len��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/doc2vec_inner.c:4365:25: note: ���__pyx_v_cum_table_len��� was declared here\n",
      "    ./gensim/models/doc2vec_inner.c:2108:151: warning: ���__pyx_v_cum_table��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/doc2vec_inner.c:4364:28: note: ���__pyx_v_cum_table��� was declared here\n",
      "    ./gensim/models/doc2vec_inner.c:1936:65: warning: ���__pyx_v_syn1��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/doc2vec_inner.c:4360:52: note: ���__pyx_v_syn1��� was declared here\n",
      "    ./gensim/models/doc2vec_inner.c: In function ���__pyx_pf_6gensim_6models_13doc2vec_inner_train_document_dbow.isra.13���:\n",
      "    ./gensim/models/doc2vec_inner.c:4053:99: warning: ���__pyx_v_syn1neg��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/doc2vec_inner.c:4053:99: warning: ���__pyx_v_cum_table_len��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/doc2vec_inner.c:4053:99: warning: ���__pyx_v_cum_table��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/doc2vec_inner.c:4031:76: warning: ���__pyx_v_syn1��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/doc2vec_inner.c: In function ���__pyx_pf_6gensim_6models_13doc2vec_inner_4train_document_dm_concat.isra.11���:\n",
      "    ./gensim/models/doc2vec_inner.c:2545:65: warning: ���__pyx_v_syn1neg��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/doc2vec_inner.c:6065:52: note: ���__pyx_v_syn1neg��� was declared here\n",
      "    ./gensim/models/doc2vec_inner.c:2487:151: warning: ���__pyx_v_cum_table_len��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/doc2vec_inner.c:6067:25: note: ���__pyx_v_cum_table_len��� was declared here\n",
      "    ./gensim/models/doc2vec_inner.c:2487:151: warning: ���__pyx_v_cum_table��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/doc2vec_inner.c:6066:28: note: ���__pyx_v_cum_table��� was declared here\n",
      "    ./gensim/models/doc2vec_inner.c:2315:65: warning: ���__pyx_v_syn1��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/doc2vec_inner.c:6062:52: note: ���__pyx_v_syn1��� was declared here\n",
      "    gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro build/temp.linux-x86_64-2.7/./gensim/models/doc2vec_inner.o -o build/lib.linux-x86_64-2.7/gensim/models/doc2vec_inner.so\n",
      "  Running setup.py install for six\n",
      "    \n",
      "    no previously-included directories found matching 'documentation/_build'\n",
      "  Running setup.py install for smart-open\n",
      "    \n",
      "  Found existing installation: boto 2.2.2\n",
      "    Uninstalling boto:\n",
      "      Successfully uninstalled boto\n",
      "  Running setup.py install for boto\n",
      "    \n",
      "    warning: no files found matching 'boto/mturk/test/*.doctest'\n",
      "    warning: no files found matching 'boto/mturk/test/.gitignore'\n",
      "    changing mode of build/scripts-2.7/sdbadmin from 644 to 755\n",
      "    changing mode of build/scripts-2.7/elbadmin from 644 to 755\n",
      "    changing mode of build/scripts-2.7/cfadmin from 644 to 755\n",
      "    changing mode of build/scripts-2.7/s3put from 644 to 755\n",
      "    changing mode of build/scripts-2.7/fetch_file from 644 to 755\n",
      "    changing mode of build/scripts-2.7/launch_instance from 644 to 755\n",
      "    changing mode of build/scripts-2.7/list_instances from 644 to 755\n",
      "    changing mode of build/scripts-2.7/taskadmin from 644 to 755\n",
      "    changing mode of build/scripts-2.7/kill_instance from 644 to 755\n",
      "    changing mode of build/scripts-2.7/bundle_image from 644 to 755\n",
      "    changing mode of build/scripts-2.7/pyami_sendmail from 644 to 755\n",
      "    changing mode of build/scripts-2.7/lss3 from 644 to 755\n",
      "    changing mode of build/scripts-2.7/cq from 644 to 755\n",
      "    changing mode of build/scripts-2.7/route53 from 644 to 755\n",
      "    changing mode of build/scripts-2.7/cwutil from 644 to 755\n",
      "    changing mode of build/scripts-2.7/instance_events from 644 to 755\n",
      "    changing mode of build/scripts-2.7/asadmin from 644 to 755\n",
      "    changing mode of build/scripts-2.7/glacier from 644 to 755\n",
      "    changing mode of build/scripts-2.7/mturk from 644 to 755\n",
      "    changing mode of build/scripts-2.7/dynamodb_dump from 644 to 755\n",
      "    changing mode of build/scripts-2.7/dynamodb_load from 644 to 755\n",
      "    changing mode of /usr/local/bin/instance_events to 755\n",
      "    changing mode of /usr/local/bin/s3put to 755\n",
      "    changing mode of /usr/local/bin/sdbadmin to 755\n",
      "    changing mode of /usr/local/bin/dynamodb_load to 755\n",
      "    changing mode of /usr/local/bin/fetch_file to 755\n",
      "    changing mode of /usr/local/bin/asadmin to 755\n",
      "    changing mode of /usr/local/bin/pyami_sendmail to 755\n",
      "    changing mode of /usr/local/bin/list_instances to 755\n",
      "    changing mode of /usr/local/bin/cfadmin to 755\n",
      "    changing mode of /usr/local/bin/lss3 to 755\n",
      "    changing mode of /usr/local/bin/launch_instance to 755\n",
      "    changing mode of /usr/local/bin/glacier to 755\n",
      "    changing mode of /usr/local/bin/bundle_image to 755\n",
      "    changing mode of /usr/local/bin/route53 to 755\n",
      "    changing mode of /usr/local/bin/elbadmin to 755\n",
      "    changing mode of /usr/local/bin/cwutil to 755\n",
      "    changing mode of /usr/local/bin/cq to 755\n",
      "    changing mode of /usr/local/bin/mturk to 755\n",
      "    changing mode of /usr/local/bin/dynamodb_dump to 755\n",
      "    changing mode of /usr/local/bin/kill_instance to 755\n",
      "    changing mode of /usr/local/bin/taskadmin to 755\n",
      "  Running setup.py install for bz2file\n",
      "    \n",
      "Successfully installed gensim six smart-open boto bz2file\n",
      "Cleaning up...\n"
     ]
    }
   ],
   "source": [
    "!sudo pip install gensim==0.10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading/unpacking nltk\n",
      "  Downloading nltk-3.0.4.tar.gz (1.0Mb): 1.0Mb downloaded\n",
      "  Running setup.py egg_info for package nltk\n",
      "    \n",
      "    warning: no files found matching 'Makefile' under directory '*.txt'\n",
      "    warning: no previously-included files matching '*~' found anywhere in distribution\n",
      "Installing collected packages: nltk\n",
      "  Running setup.py install for nltk\n",
      "    \n",
      "    warning: no files found matching 'Makefile' under directory '*.txt'\n",
      "    warning: no previously-included files matching '*~' found anywhere in distribution\n",
      "Successfully installed nltk\n",
      "Cleaning up...\n"
     ]
    }
   ],
   "source": [
    "!sudo pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading/unpacking python-cjson\n",
      "  Downloading python-cjson-1.1.0.tar.gz\n",
      "  Running setup.py egg_info for package python-cjson\n",
      "    \n",
      "Installing collected packages: python-cjson\n",
      "  Running setup.py install for python-cjson\n",
      "    building 'cjson' extension\n",
      "    gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DMODULE_VERSION=1.1.0 -I/usr/include/python2.7 -c cjson.c -o build/temp.linux-x86_64-2.7/cjson.o\n",
      "    gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro build/temp.linux-x86_64-2.7/cjson.o -o build/lib.linux-x86_64-2.7/cjson.so\n",
      "    \n",
      "Successfully installed python-cjson\n",
      "Cleaning up...\n"
     ]
    }
   ],
   "source": [
    "!sudo pip install python-cjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection u'all-corpora'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package oanc_masc to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package omw to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pil to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package ptb to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package qc to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package rte to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package udhr to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all-corpora\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/scikits/__init__.py:1: UserWarning: Module boto was already imported from /usr/lib/python2.7/dist-packages/boto/__init__.pyc, but /usr/local/lib/python2.7/dist-packages is being added to sys.path\n",
      "  __import__('pkg_resources').declare_namespace(__name__)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all-corpora')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE WORD CLEAN AND TOKENIZE FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords.append('[deleted]')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def text_cleaner(text):\n",
    "    '''\n",
    "    INPUT: string of body text\n",
    "    OUTPUT: List of tokenized lower case words with stopwords removed\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # Output tokenizes text and removes any stopwords and then outptus lowercased words\n",
    "    return [word.lower() for word in tokenizer.tokenize(text) if not word.lower() in stopwords]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to generate labeled sentences straight from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cjson\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "import gensim.models.doc2vec\n",
    "\n",
    "\n",
    "def reddit_comment_gen(pathway):\n",
    "    '''\n",
    "\tINPUT: Pathway to database and num of comments to be generated. If everything is True, all comments returned.\n",
    "\tOUTPUT: Generator label and tokenized comment\n",
    "\n",
    "\t'''\n",
    "\n",
    "    ## Generate all labeled sentences from file\n",
    "\n",
    "    # \n",
    "    \n",
    "   \n",
    "        # Iterate through N JSON objects in file\n",
    "    with open(pathway) as myfile:\n",
    "        for item in myfile:\n",
    "            \n",
    "            \n",
    "            # put in try statement here\n",
    "\t                \n",
    "            # Load the JSON object\n",
    "            json_object = cjson.decode(item)\n",
    "\n",
    "            # Clean and tokenize text\n",
    "            body = text_cleaner(json_object['body'])\n",
    "\n",
    "            # generate\n",
    "            yield LabeledSentence(body,labels=[str(json_object['subreddit'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The function that builds the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "def build_model(pathway):\n",
    "\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    assert gensim.models.doc2vec.FAST_VERSION > -1\n",
    "\n",
    "    \n",
    "    d2v_reddit_model = Doc2Vec( dm=0, size=100, negative=5, hs=0, min_count=2, workers=cores)\n",
    "    d2v_reddit_model.train_words = False\n",
    "    \n",
    "    d2v_reddit_model.build_vocab(reddit_comment_gen(pathway)) #sentence_gen(reddit_data))\n",
    "\n",
    "\n",
    "    #d2v_reddit_model.train(reddit_comment_gen(pathway,10000)) #sentence_gen(reddit_data))\n",
    "\n",
    "    for epoch in range(10):\n",
    "\n",
    "        d2v_reddit_model.train(reddit_comment_gen(pathway))\n",
    "        d2v_reddit_model.alpha -= 0.002  # decrease the learning rate\n",
    "        d2v_reddit_model.min_alpha = d2v_reddit_model.alpha  # fix the learning rate, no decay\n",
    "\n",
    "    return d2v_reddit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<boto.s3.bucketlistresultset.BucketListResultSet instance at 0x409bb48>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto\n",
    "from boto.s3.connection import S3Connection\n",
    "\n",
    "# Get connection with keys (may need to input these keys)\n",
    "conn = S3Connection(aws_access_key_id='AKIAJE74CVWSMVH3FBCQ',aws_secret_access_key='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')\n",
    "\n",
    "# Connect to exsisting bucket\n",
    "mybucket = conn.get_bucket('testreddit')\n",
    "\n",
    "# List keys in the bucket\n",
    "mybucket.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from boto.s3.key import Key\n",
    "\n",
    "k = Key(mybucket)\n",
    "\n",
    "k.key = 'sample99copy.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k.get_contents_to_filename('my_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_model = build_model('my_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'image', 0.2593920826911926),\n",
       " ('decent', 0.25427594780921936),\n",
       " ('hard', 0.2527804970741272),\n",
       " ('might', 0.25075313448905945),\n",
       " ('unleaded', 0.23790906369686127),\n",
       " ('comfortable', 0.20254279673099518),\n",
       " (u'brutalism', 0.20020368695259094),\n",
       " ('haha', 0.19362273812294006),\n",
       " ('instead', 0.192079097032547),\n",
       " ('got', 0.1804526150226593)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.most_similar('thank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
